{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNNvcPAgbsOhHfMgBJy6/FO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[""],"metadata":{"id":"Gwahmp-SB5fm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0C4oPEIwBxl0"},"outputs":[],"source":["\"\"\"\n","Glossary:\n","Autoregressive model - a sliding window sequence model, can literally be as simple as a MLP that\n","outputs a result and then you just input lastInput[1:] + last output\n","K-step ahead prediction - the amount ahead you're generating with your model\n","Tokenization - tokenizing the text into numbers so the sequence model can read it\n","Maximization of P(x1,x2,...xt) - language models are built to approximate the joint\n","probability of seeing those tokens together, they can also then generate\n","Usefulness of simple language models - simple language models can still be useful,\n","for instance, if a speech recognition system gives two answers, a language model\n","could check them over and determine which one is more likely.\n","Data preparation - it seems that quite a lot of time goes into data preparation\n","and data design. Which makes sense from what I've experienced\n","Log log graph - a graph where both the x and y axis is log, apparently its good\n","for checking if there's a power relationship, ie y = ax**k, and then you can\n","determine k and a from the slope and intercept respectively. Power graphs grow\n","slower than exponential graphs\n","Latent Variable - in statistical models, a latent variable is one that you infer\n","from variables that you can observe. For instance the emission probabilities of\n","hidden states in a HMM, or perhaps the sequence of the HMM\n","Manifest Variable - another word for observable variable\n","Latent Variable Model - a model that relates observable (manifest) variables to\n","latent variables. Any hidden layer would be considered a latent variable. Well\n","apparently they're supposed to be different, but lets be real they're not\n","Latent Variable Hidden Layer - the hidden layer for a latent variable model has\n","weights for both the current observation and the hidden variables before. I \n","suppose the reasoning for saying that different must come from the minutia of\n","the implementation and training\n","Recurrent Neural Network - a deep latent variable model in which Ht is some function\n","of Xt and Ht-1, with the only important part being that the function is the same\n","as each time step, just with different parameters. Therefore it's recurrent. Much\n","like recursion\n","Recurrent Layer - layers that perform the recursive function\n","On Recurrent Networks - the power is in the fact that unlike autoregressive models,\n","you don't need to feed in multiple observations, and the predictive power isn't\n","limited just to these obvservations either. A latent variable model only needs\n","the current observation and it can integrate all previous observations using its\n","recurrence function\n","Vanishing Gradient - as you backpropogate further through a RNN, the gradient\n","gets smaller. Without implementing, I believe this is because of the local\n","gradient and the weights being 0-1 usually\n","\"\"\""]},{"cell_type":"code","source":["\"\"\"\n","Pseudo-\n","Bring in data\n","Tokenize and break up into batches\n","Construct model -> ht = o(ht-1*hiddenWeights + xt*observation weights + b)\n","                -> Out = ht*outputweights\n","Train on model\n","\"\"\""],"metadata":{"id":"gLJaI7X5B4mB"},"execution_count":null,"outputs":[]}]}